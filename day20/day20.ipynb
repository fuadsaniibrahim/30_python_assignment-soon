{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DAY 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser \n",
    "\n",
    "\n",
    "url_lists = [\n",
    "    'http://www.python.org',\n",
    "    'https://www.linkedin.com/in/asabeneh/',\n",
    "    'https://github.com/Asabeneh',\n",
    "    'https://twitter.com/Asabeneh',\n",
    "]\n",
    "\n",
    "#\n",
    "for url in url_lists:\n",
    "    webbrowser.open_new_tab(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # importing the request module\n",
    "\n",
    "url = 'https://www.w3.org/TR/PNG/iso_8859-1.txt' # text from a website\n",
    "\n",
    "response = requests.get(url) # opening a network and fetching a data\n",
    "print(response)\n",
    "print(response.status_code) # status code, success:200\n",
    "print(response.headers)     # headers information\n",
    "print(response.text) # gives all the text from the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'https://restcountries.eu/rest/v2/all'  # countries api\n",
    "response = requests.get(url)  # opening a network and fetching a data\n",
    "print(response) # response object\n",
    "print(response.status_code)  # status code, success:200\n",
    "countries = response.json()\n",
    "print(countries[:1])  # we sliced only the first country, remove the slicing to see all countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 5), ('you', 4), ('project', 3), ('gutenberg', 3), ('about', 3), ('contact', 3), ('of', 3), ('and', 3), ('help', 3), ('to', 3)]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "url = 'http://www.gutenberg.org/files/1112/1112.txt'\n",
    "response = requests.get(url)\n",
    "\n",
    "content = response.text\n",
    "soup = BeautifulSoup(content,'html.parser')\n",
    "cleaned = soup.get_text()\n",
    "\n",
    "cleaned = re.sub(r'[^a-zA-Z\\s]','',cleaned.lower())\n",
    "\n",
    "words = cleaned.split()\n",
    "count = Counter(words)\n",
    "\n",
    "most_frequent = count.most_common(10)\n",
    "print(most_frequent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the min, max, mean, median, standard deviation of cats' weight are 3.0,7.5,4.708955223880597,4.5,1.0585446702386354 respectively\n",
      "the min, max, mean, median, standard deviation of cats' lifespan are 10.5,19.0,13.746268656716419,13.5,1.5725564658451314 respectively\n",
      "           origin              name  count\n",
      "0       Australia   Australian Mist      1\n",
      "1           Burma           Burmese      1\n",
      "2           Burma  European Burmese      1\n",
      "3          Canada            Cymric      1\n",
      "4          Canada            Sphynx      1\n",
      "..            ...               ...    ...\n",
      "62  United States          Savannah      1\n",
      "63  United States       Selkirk Rex      1\n",
      "64  United States          Snowshoe      1\n",
      "65  United States            Toyger      1\n",
      "66  United States    York Chocolate      1\n",
      "\n",
      "[67 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statistics import mean, median, stdev\n",
    "\n",
    "\n",
    "cat_api = 'https://api.thecatapi.com/v1/breeds'\n",
    "response = requests.get(cat_api)\n",
    "cat = response.json()\n",
    "\n",
    "cat_df = pd.DataFrame(cat)\n",
    "\n",
    "metric_weights = []\n",
    "# Create a DataFrame from the API response\n",
    "\n",
    "life_span = []\n",
    "for i in cat:\n",
    "        metric_weights.append(\n",
    "            (int(i['weight']['metric'].split()[0]) + int(i['weight']['metric'].split()[-1])) / 2)\n",
    "        life_span.append((int(i['life_span'].split()[0]) + int(i['life_span'].split()[-1])) / 2)\n",
    "\n",
    "\n",
    "median_m = np.median(metric_weights)\n",
    "mean_m = np.mean(metric_weights)\n",
    "min_m = min(metric_weights)\n",
    "max_m = max(metric_weights)\n",
    "sd_m = np.std(metric_weights)\n",
    "\n",
    "\n",
    "median_l = np.median(life_span)\n",
    "mean_l = np.mean(life_span)\n",
    "min_l = min(life_span)\n",
    "max_l = max(life_span)\n",
    "sd_l = np.std(life_span)\n",
    "\n",
    "\n",
    "print(f'the min, max, mean, median, standard deviation of cats\\' weight are {min_m},{max_m},{mean_m},{median_m},{sd_m} respectively' )\n",
    "print(f'the min, max, mean, median, standard deviation of cats\\' lifespan are {min_l},{max_l},{mean_l},{median_l},{sd_l} respectively' )\n",
    "\n",
    "countries = {}\n",
    "\n",
    "\n",
    "for i in cat:\n",
    "        countries[i['origin']] = countries.get(i['origin'], 0) + 1 \n",
    "\n",
    "frequency_table = cats_df.groupby(['origin', 'name']).size().reset_index(name='count')\n",
    "\n",
    "\n",
    "print(frequency_table)\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "uci machine learning repository\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "iris  uci machine learning repository\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "       datasets contribute dataset donate new link external about us who we are citation metadata contact information           login       iris  donated on      a small classic dataset from fisher  one of the earliest known datasets used for evaluating classification methods\n",
      "  dataset characteristics tabular subject area biology associated tasks classification feature type real  instances   features    dataset information    what do the instances in this dataset represent  each instance is a plant  additional information  this is one of the earliest datasets used in the literature on classification methods and widely used in statistics and machine learning  the data set contains  classes of  instances each where each class refers to a type of iris plant  one class is linearly separable from the other  the latter are not linearly separable from each other\n",
      "\n",
      "predicted attribute class of iris plant\n",
      "\n",
      "this is an exceedingly simple domain\n",
      "\n",
      "this data differs from the data presented in fishers article identified by steve chadwick  spchadwickespeedaznet   the th sample should be irissetosa where the error is in the fourth feature the th sample irissetosa where the errors are in the second and third features    has missing values  no    introductory paper    the iris data set in search of the source of virginica by a unwin k kleinman  published in significance   variables table      variable nameroletypedemographicdescriptionunitsmissing values sepal lengthfeaturecontinuouscmno sepal widthfeaturecontinuouscmno petal lengthfeaturecontinuouscmno petal widthfeaturecontinuouscmno classtargetcategoricalclass of iris plant iris setosa iris versicolour or iris virginicano  rows per page   to  of     baseline model performance     accuracy precision     papers citing this dataset     sort by year desc title  year   venue  journal    a constructive approach for oneshot training of neural networks using hypercubebased topological coverings by w daniel enoch yeung  published in arxiv  convergence and margin of adversarial training on separable data by zachary charles shashank rajput stephen wright dimitris papailiopoulos  published in arxiv  crad clustering with robust autocuts and depth by xin huang yulia gel  published in  ieee international conference on data mining icdm    deep spiking neural network with spike count based learning rule by jibin wu yansong chua malu zhang qu yang guoqi li haizhou li  published in arxiv  bounded fuzzy possibilistic method by hossein yazdani  published in arxiv   rows per page   to  of     reviews    there are no reviews for this dataset yet login to write a review   write a review     comments \n",
      "  submit cancel    download  import in python  install the ucimlrepo package    pip install ucimlrepo import the dataset into your code    from ucimlrepo import fetchucirepo \n",
      "  \n",
      " fetch dataset \n",
      "iris  fetchucirepoid \n",
      "  \n",
      " data as pandas dataframes \n",
      "x  irisdatafeatures \n",
      "y  irisdatatargets \n",
      "  \n",
      " metadata \n",
      "printirismetadata \n",
      "  \n",
      " variable information \n",
      "printirisvariables \n",
      " view the full documentation cite    citations   views    citation   fisherr a  iris uci machine learning repository httpsdoiorgcc   bibtex   miscmisciris\n",
      "  author        fisherr a\n",
      "  title         iris\n",
      "  year          \n",
      "  howpublished  uci machine learning repository\n",
      "  note          doi httpsdoiorgcc\n",
      "    keywords  ecology   creators    r a fisher     doi cc  license this dataset is licensed under a\n",
      "creative commons attribution  international\n",
      "cc by  license this allows for the sharing and adaptation of the datasets for any purpose\n",
      "provided that the appropriate credit is given  by using the uci machine learning repository\n",
      "you acknowledge and accept the cookies and privacy practices used by the uci machine learning repository accept read policy  the project about us cml national science foundation navigation home view datasets donate a dataset logistics contact privacy notice feature request or bug report  browse datasets donate a dataset link an external dataset  who we are citation metadata contact information   login \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "api = 'https://archive.ics.uci.edu/dataset/53/iris'\n",
    "\n",
    "response = requests.get(api)\n",
    "\n",
    "countries = response.text\n",
    "soup = BeautifulSoup(countries, 'html.parser')\n",
    "\n",
    "clean = soup.text\n",
    "clean = re.sub(r'[^a-zA-Z\\s]','',clean.lower())\n",
    "print(clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titles of Datasets:\n"
     ]
    }
   ],
   "source": [
    "uci_url = 'https://archive.ics.uci.edu/datasets'\n",
    "response = requests.get(uci_url)\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    dataset_titles = soup.find_all('p', class_='normal')\n",
    "    \n",
    "    print(\"Titles of Datasets:\")\n",
    "    for title in dataset_titles:\n",
    "        print(title.text.strip())\n",
    "\n",
    "    \n",
    "else:\n",
    "    print(f\"Failed to fetch data from UCI. Status code: {response.status_code}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArewaDS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
